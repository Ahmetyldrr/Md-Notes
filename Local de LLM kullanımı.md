## Yerel (Local) LLM Ã‡alÄ±ÅŸtÄ±rma Rehberi  
*TÃ¼rkÃ§e konuÅŸanlar iÃ§in hazÄ±rlanmÄ±ÅŸtÄ±r.  Her adÄ±mda kÄ±sa aÃ§Ä±klamalar ve Ã¶rnek komutlar bulacaksÄ±nÄ±z.*

---

## 1. Neye Ä°htiyacÄ±nÄ±z Var?

| BileÅŸen | Minimum Gereksinim | Ã–nerilen/Ä°deal |
|---------|-------------------|----------------|
| **CPU** | 8â€‘core (Intel i7 / AMD Ryzen 7) | 12â€‘core+ |
| **GPU** | 8â€¯GB VRAM (NVIDIA GTXâ€¯1660â€‘Ti vb.) | 12â€‘24â€¯GB VRAM (RTXâ€¯3080/3090, RTXâ€¯A6000, RTXâ€¯4060â€‘Ti 16â€¯GB vb.) |
| **RAM** | 16â€¯GB | 32â€¯GB+ |
| **Depolama** | 100â€¯GB SSD (model dosyalarÄ± sÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ) | 500â€¯GB+ NVMe (Ã¶zellikle 70â€¯GBâ€‘100â€¯GB modeller iÃ§in) |
| **OS** | Windowsâ€¯10/11, Ubuntuâ€¯20.04+, macOSâ€¯12+ (Apple Silicon iÃ§in farklÄ± Ã§Ã¶zÃ¼mler) |
| **Software** | â€¢ Pythonâ€¯3.9â€‘3.11  <br>â€¢ CUDAâ€¯12.x (GPUâ€™nuz NVIDIA ise) <br>â€¢ PyTorch (CUDA destekli) <br>â€¢ Git  <br>â€¢ (Ä°steÄŸe baÄŸlÄ±) Docker, Conda, VSâ€¯Code |

> **Not:** Model lisanslarÄ± ve kullanÄ±m koÅŸullarÄ± dikkatle incelenmelidir (Meta LLaMA, Mistralâ€‘7B, Falconâ€‘180B gibi). AÃ§Ä±kâ€‘kaynak ve ticari olmayan modellerle Ã§alÄ±ÅŸmak genellikle sorunsuzdur.

---

## 2. Ã‡alÄ±ÅŸma OrtamÄ±nÄ± HazÄ±rlama

### 2.1 Python ve Sanal Ortam

```bash
# Ubuntu / macOS
sudo apt-get update && sudo apt-get install -y python3-venv git

# Sanal ortam oluÅŸtur
python3 -m venv llm-env
source llm-env/bin/activate   # Windows: .\llm-env\Scripts\activate

# pip'i gÃ¼ncelle
pip install --upgrade pip setuptools wheel
```

### 2.2 CUDA ve PyTorch (GPU KullanÄ±mÄ±)

```bash
# CUDA sÃ¼rÃ¼mÃ¼nÃ¼zÃ¼ Ã¶ÄŸrenin (nvidia-smi)
nvidia-smi
# Ã‡Ä±ktÄ± Ã¶rneÄŸi: CUDA Version: 12.2

# PyTorchâ€™u uygun CUDA versiyonu ile kurun
pip install torch==2.3.0+cu122 torchvision==0.18.0+cu122 torchaudio==2.3.0+cu122 \
    -f https://download.pytorch.org/whl/torch_stable.html
```

> **Alternatif:** EÄŸer GPU yoksa **CPUâ€‘only** PyTorch kurabilirsiniz:
> `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu`

### 2.3 DiÄŸer Gereksinimler

```bash
pip install transformers==4.41.0 \
    accelerate==0.30.1 \
    bitsandbytes==0.43.1 \
    einops sentencepiece tqdm
```

- **`bitsandbytes`**: 4â€‘bit / 8â€‘bit quantization (GPUâ€‘RAM tasarrufu).  
- **`accelerate`**: Ã‡oklu GPU / CPU daÄŸÄ±tÄ±mÄ± iÃ§in yardÄ±mcÄ± araÃ§.

---

## 3. Hangi Modeli KullanacaksÄ±nÄ±z?

| Model | Parametre | Ã–nerilen VRAM | Lisans | KullanÄ±m Ã–rneÄŸi |
|-------|-----------|---------------|-------|-----------------|
| **LLaMAâ€‘2 7B** | 7â€¯B | 8â€¯GB (4â€‘bit) / 12â€¯GB (FP16) | Meta Research (nonâ€‘commercial) | `meta-llama/Llama-2-7b-hf` (HuggingFace) |
| **Mistralâ€‘7Bâ€‘Instruct** | 7â€¯B | 8â€¯GB (4â€‘bit) | Apacheâ€‘2.0 | `mistralai/Mistral-7B-Instruct-v0.2` |
| **Falconâ€‘7Bâ€‘Instruct** | 7â€¯B | 8â€¯GB (4â€‘bit) | Apacheâ€‘2.0 | `tiiuae/falcon-7b-instruct` |
| **Phiâ€‘3â€‘miniâ€‘4kâ€‘instruct** | 3.8â€¯B | 4â€¯GB (FP16) | Apacheâ€‘2.0 | `microsoft/phi-3-mini-4k-instruct` |
| **Gemmaâ€‘2â€‘9B** | 9â€¯B | 12â€¯GB (FP16) | Apacheâ€‘2.0 | `google/gemma-2-9b` |

> **KÄ±sa Not:** 4â€‘bit quantization (`bitsandbytes`) ile 7â€¯B modelini 8â€¯GB VRAMâ€™da rahatÃ§a Ã§alÄ±ÅŸtÄ±rabilirsiniz. 12â€¯GB+ VRAM varsa FP16 (halfâ€‘precision) tercih edin, daha stabil ve hÄ±zlÄ± olur.

---

## 4. Modeli Ä°ndirme & Quantize Etme

### 4.1 HuggingFace Hubâ€™dan Ä°ndirme

```bash
git lfs install
git clone https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
cd Mistral-7B-Instruct-v0.2
```

> **Alternatif (direct `transformers` download):**
> ```python
> from transformers import AutoModelForCausalLM, AutoTokenizer
> model_name = "mistralai/Mistral-7B-Instruct-v0.2"
> tokenizer = AutoTokenizer.from_pretrained(model_name)
> model = AutoModelForCausalLM.from_pretrained(model_name)
> ```

### 4.2 4â€‘bit Quantization (bitsandbytes)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = "mistralai/Mistral-7B-Instruct-v0.2"

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,          # 4â€‘bit quantization
    bnb_4bit_compute_dtype="float16",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",  # nâ€‘f4 (en iyi kaliteâ€‘performans dengesi)
)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",          # GPU/CPU otomatik daÄŸÄ±tÄ±m
    quantization_config=quant_config,
    trust_remote_code=True,
)
```

**`device_map="auto"`** â†’ Model parÃ§alarÄ± otomatik olarak GPUâ€‘VRAMâ€™a sÄ±ÄŸacak ÅŸekilde yerleÅŸtirilir. Tek GPUâ€™da Ã§alÄ±ÅŸÄ±yorsanÄ±z `"balanced"` veya `"sequential"` da kullanÄ±labilir.

### 4.3 8â€‘bit Quantization (daha az kalite, daha hÄ±zlÄ±)

```python
quant_config = BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype="float16"
)
```

---

## 5. Basit Inference (Tahmin) Ã–rneÄŸi

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_name = "mistralai/Mistral-7B-Instruct-v0.2"

# 4â€‘bit quantization yapÄ±landÄ±rmasÄ±
quant_cfg = BitsAndBytesConfig(load_in_4bit=True,
                               bnb_4bit_compute_dtype="float16",
                               bnb_4bit_use_double_quant=True,
                               bnb_4bit_quant_type="nf4")

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    quantization_config=quant_cfg,
    trust_remote_code=True,
)

def ask(prompt: str, max_new_tokens: int = 256):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            eos_token_id=tokenizer.eos_token_id,
        )
    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return answer

# Ã–rnek kullanÄ±m
print(ask("TÃ¼rkiye'nin baÅŸkenti neresidir?"))
```

**Ã‡Ä±ktÄ± (Ã¶rnek):**
```
TÃ¼rkiye'nin baÅŸkenti Ankara'dÄ±r. ...
```

---

## 6. Performans & Bellek YÃ¶netimi Ä°puÃ§larÄ±

| Teknik | AÃ§Ä±klama | Ne Zaman KullanmalÄ±? |
|--------|----------|----------------------|
| **`torch.cuda.empty_cache()`** | BoÅŸ GPU bellek bloklarÄ±nÄ± temizler. | Tekrarlayan inference sÄ±rasÄ±nda hafÄ±za biriktiÄŸinde |
| **`torch.compile()` (PyTorchâ€¯2.0+)** | JITâ€‘aÃ§Ä±lÄ±m, hÄ±z artÄ±ÅŸÄ± (â‰ˆ%20â€‘30). | Model bÃ¼yÃ¼kse ve CPU/GPU bÃ¼yÃ¼k derecede meÅŸgulse |
| **`accelerate launch`** | Ã‡oklu GPU daÄŸÄ±tÄ±mÄ± ve mixedâ€‘precision otomatik. | 2â€‘4+ GPUâ€™ya sahipseniz |
| **`llama.cpp` / `ggml`** | Câ€‘tabanlÄ±, yalnÄ±zca CPU veya dÃ¼ÅŸÃ¼kâ€‘VRAM GPUâ€™da Ã§alÄ±ÅŸÄ±r, 4â€‘bit/8â€‘bit quant. | GPU yoksa veya Ã§ok dÃ¼ÅŸÃ¼k VRAM (â‰¤4â€¯GB) varsa |
| **`Ollama` veya `LM Studio`** | GUI + API, modelleri tek komutla kurar. | HÄ±zlÄ± deneme ve UI istiyorsanÄ±z |

---

## 7. GUI / API Ã‡Ã¶zÃ¼mleri (Opsiyonel)

### 7.1 LM Studio (Ãœcretsiz, Ã‡aprazâ€‘platform)

1. <https://lmstudio.ai> sitesinden DMG (macOS), EXE (Windows) ya da AppImage (Linux) indirin.  
2. Kurulum sonrasÄ± â€œModel Galleryâ€den **Mistralâ€‘7Bâ€‘Instruct**, **LLaMAâ€‘2â€‘7B** vb. indirect olarak seÃ§ip â€œDownload & Loadâ€e tÄ±klayÄ±n.  
3. **Local Server** (REST/HTTPS) Ã¶zelliÄŸini aÃ§Ä±n â†’ `http://127.0.0.1:1234/v1/chat/completions` gibi bir endpoint alÄ±rsÄ±nÄ±z.  
4. Pythonâ€™da `openai` paketini kullanarak Ã§aÄŸrÄ± yapabilirsiniz:

```python
import openai

client = openai.OpenAI(base_url="http://127.0.0.1:1234/v1")
resp = client.chat.completions.create(
    model="mistral-7b-instruct",
    messages=[{"role":"user","content":"TÃ¼rk tarihindeki en Ã¶nemli 3 olay nedir?"}]
)
print(resp.choices[0].message.content)
```

### 7.2 Ollama (Docker + CLI)

```bash
# Linux/macOS (Docker kurulu)
curl -fsSL https://ollama.com/install.sh | sh

# Windows (PowerShell)
iwr -useb https://ollama.com/install.ps1 | iex

# Modeli indir (Ã¶rn. llama2)
ollama pull llama2

# Basit CLI kullanÄ±m
ollama run llama2
```

Ollama, `ollama serve` komutuyla bir OpenAIâ€‘compatible API sunar â†’ aynÄ± `openai` kodu Ã§alÄ±ÅŸÄ±r.

---

## 8. Model LisanslarÄ± ve Etik Kurallar

1. **Meta LLaMAâ€‘2** â€“ yalnÄ±zca â€œnonâ€‘commercialâ€ (ticari olmayan) kullanÄ±m izni.  
2. **Mistralâ€‘7B, Falconâ€‘7B, Phiâ€‘3** â€“ Apacheâ€‘2.0 (Ã¶zgÃ¼r; ticari de dahil).  
3. **Gemmaâ€‘2** â€“ Googleâ€™Ä±n Apacheâ€‘2.0 lisansÄ± (ticari).  
4. **Veri gizliliÄŸi** â€“ KullanÄ±cÄ± verilerini kaydettiÄŸiniz bir servis (Ã¶r. bir web UI) yapacaksanÄ±z GDPR/KVK gibi yasalarÄ± kontrol edin.  
5. **Telif hakkÄ± / Ä°Ã§erik** â€“ Model Ã§Ä±ktÄ±larÄ± telifli materyaller iÃ§eriyorsa sorumlu kullanÄ±m ilkelerini uygulayÄ±n.

---

## 9. Sorun Giderme (Common Issues)

| Hata | Muhtemel Neden | Ã‡Ã¶zÃ¼m |
|------|----------------|-------|
| `RuntimeError: CUDA out of memory` | Model VRAM limitini aÅŸÄ±yor. | - 4â€‘bit quantize edin (`load_in_4bit=True`). <br> - `device_map="auto"` yerine `"cpu"`/`"balanced"` deneyin. <br> - `max_new_tokens` deÄŸerini dÃ¼ÅŸÃ¼rÃ¼n. |
| `bitsandbytes not compiled with CUDA` | `bitsandbytes` GPU sÃ¼rÃ¼mÃ¼ yÃ¼klenmemiÅŸ. | `pip uninstall bitsandbytes && pip install bitsandbytes-cu122` (CUDAâ€¯12.2 Ã¶rneÄŸi). |
| `OSError: [Errno 2] No such file or directory: 'git'` | Git yÃ¼klÃ¼ deÄŸil. | `sudo apt install git` (Linux) / Git for Windows kurun. |
| `ImportError: cannot import name 'AutoModelForCausalLM' from 'transformers'` | Transformers Ã§ok eski. | `pip install -U transformers`. |
| `UserWarning: The tokenizers library etc.` | Tokenizer modeli eksik. | `pip install tokenizers` ya da `pip install sentencepiece` (modelin ihtiyacÄ±na gÃ¶re). |

---

## 10. HÄ±zlÄ± BaÅŸlangÄ±Ã§ â€“ Tek Komutlu Script Ã–rneÄŸi (Linux/macOS)

```bash
#!/usr/bin/env bash
set -e

# 1ï¸âƒ£ Ortam oluÅŸtur
python3 -m venv llm && source llm/bin/activate

# 2ï¸âƒ£ Gereksinimler
pip install --upgrade pip
pip install torch==2.3.0+cu122 torchvision==0.18.0+cu122 torchaudio==2.3.0+cu122 \
    -f https://download.pytorch.org/whl/torch_stable.html
pip install transformers accelerate bitsandbytes einops sentencepiece tqdm

# 3ï¸âƒ£ Modeli indir ve quantize et (Mistralâ€‘7Bâ€‘Instruct)
cat > infer.py <<'PY'
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_name = "mistralai/Mistral-7B-Instruct-v0.2"
quant_cfg = BitsAndBytesConfig(load_in_4bit=True,
                               bnb_4bit_compute_dtype="float16",
                               bnb_4bit_use_double_quant=True,
                               bnb_4bit_quant_type="nf4")

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    quantization_config=quant_cfg,
    trust_remote_code=True,
)

def chat(prompt: str):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        out = model.generate(**inputs, max_new_tokens=200, temperature=0.7, top_p=0.9, do_sample=True)
    return tokenizer.decode(out[0], skip_special_tokens=True)

print(chat("TÃ¼rk mutfaÄŸÄ±nÄ±n en meÅŸhur 5 yemeÄŸi hangileridir?"))
PY

# 4ï¸âƒ£ Ã‡alÄ±ÅŸtÄ±r
python infer.py
```

Bu scriptâ€™i bir dosyaya (`install_and_run.sh`) yapÄ±ÅŸtÄ±rÄ±p `bash install_and_run.sh` komutuyla Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zda:

- Sanal ortam oluÅŸturulur,
- PyTorch + CUDAâ€‘destekli paketler kurulur,
- `bitsandbytes` ile 4â€‘bit quantize edilmiÅŸ Mistralâ€‘7Bâ€‘Instruct modeli indirilir,
- Tek bir soruya yanÄ±t alÄ±nÄ±r.

---

## 11. Daha Ä°leri Konular (Ä°steÄŸe BaÄŸlÄ±)

| Konu | AÃ§Ä±klama | Kaynak |
|------|----------|--------|
| **LoRA / PEFT ile Ä°nce Ayar (Fineâ€‘tune)** | KÃ¼Ã§Ã¼k veri setiyle Ã¶zel gÃ¶revler iÃ§in 0.5â€‘2â€¯GB parametre ekleyin. | `peft` kÃ¼tÃ¼phanesi, <https://github.com/huggingface/peft> |
| **GPUâ€‘offload / CPUâ€‘offload** | BÃ¼yÃ¼k modelleri (13â€‘30â€¯B) **CPUâ€‘GPU hibrid** yÃ¼rÃ¼tme ile Ã§alÄ±ÅŸtÄ±rÄ±n. | `accelerate config` â†’ `offload` seÃ§eneÄŸi |
| **Distributed Inference** | Ã‡oklu sunucu/ GPU Ã¼zerinden artan throughput. | `vLLM` (<https://github.com/vllm-project/vllm>) |
| **OpenAIâ€‘compatible API** | Yerel sunucu ile `openai` paketini tamamen aynÄ± ÅŸekilde kullanÄ±n. | `text-generation-webui`, `LM Studio`, `Ollama` |
| **RAG (Retrievalâ€‘Augmented Generation)** | DÄ±ÅŸ veri kaynaklarÄ±ndan bilgi Ã§ekip modele â€œpromptâ€ ekleyin. | Haystack, LangChain, Llamaâ€‘Index |

---

## 12. Son SÃ¶z

- **Deneme yanÄ±lma** en hÄ±zlÄ± Ã¶ÄŸrenme yoludur. KÃ¼Ã§Ã¼k bir 3â€‘B model (Phiâ€‘3â€‘mini) ile baÅŸlayÄ±n; CUDAâ€‘memoria ve quantization ayarlarÄ±nÄ± kavrayÄ±n.  
- **Model seÃ§imi** amacÄ±nÄ±za gÃ¶re deÄŸiÅŸir: sohbet, kod Ã¼retimi, bilimsel metin, vs.  
- **GPU sÃ¼rÃ¼cÃ¼sÃ¼ ve CUDA** sÃ¼rÃ¼mÃ¼nÃ¼n uyumlu olduÄŸundan emin olun; Ã§oÄŸu zaman `nvidia-smi` ve `nvcc --version` Ã§Ä±ktÄ±larÄ±yla kontrol edebilirsiniz.

# Ä°kinci BÃ¶lÃ¼m

## ğŸ¯ â€œLLMâ€™i yerel (local) makinede Ã§alÄ±ÅŸtÄ±rmakâ€ iÃ§in 0â€™danâ€¯1â€™e adÄ±mâ€‘adÄ±m rehber  
*(TÃ¼rkÃ§e aÃ§Ä±klamalar, komut satÄ±rÄ± ve Python kodlarÄ± dahil)*  

> **Not:** AÅŸaÄŸÄ±daki adÄ±mlar **Linux/macOS** ve **Windows** (PowerShell/WSL) ortamlarÄ± iÃ§in geÃ§erlidir.  
> Ã‡oÄŸu modern LLMâ€™i **GPU** (CUDA) ile Ã§ok daha hÄ±zlÄ± Ã§alÄ±ÅŸÄ±r. EÄŸer GPUâ€™nuz yoksa â€œCPUâ€‘onlyâ€ versiyonlarÄ±nÄ± da kullanabilirsiniz â€“ bu durumda biraz daha uzun yanÄ±t sÃ¼resi olur.

---

## 1ï¸âƒ£ Sistem Gereksinimleri

| BileÅŸen | Minimum | Ã–nerilen |
|--------|----------|----------|
| **Ä°ÅŸletim Sistemi** | 64â€‘bit Linux/macOS/Windows | Linux (Ubuntuâ€¯22.04) en stabil |
| **CPU** | 4â€‘Ã§ekirdek | 8â€‘Ã§ekirdek+ |
| **RAM** | 8â€¯GB | 16â€‘32â€¯GB |
| **GPU** (isteÄŸe baÄŸlÄ±) | Yok (CPUâ€‘only) | NVIDIA RTXâ€¯3060â€¯+ (12â€¯GB VRAM) veya daha yÃ¼ksek |
| **Disk** | 10â€¯GB boÅŸ | 30â€‘50â€¯GB (model aÄŸÄ±rlÄ±klarÄ± iÃ§in) |
| **Python** | 3.9â€‘3.11 | 3.10 Ã¶nerilir |

> **GPUâ€‘destekli kurulumda**: **CUDA Toolkit** (â‰¥â€¯11.7) ve **cuDNN** yÃ¼klÃ¼ olmalÄ±. `nvidia-smi` komutu GPUâ€™nun dÃ¼zgÃ¼n kurulduÄŸunu gÃ¶sterir.

---

## 2ï¸âƒ£ Ortam (Environment) OluÅŸturma

### Conda (Ã¶nerilen)

```bash
# 1. Conda (Miniconda/Anaconda) kurulu olduÄŸunu varsayÄ±yoruz
conda create -n llm_local python=3.10 -y
conda activate llm_local
```

### Venv (alternatif)

```bash
python -m venv llm_local
source llm_local/bin/activate   # Linux/macOS
.\llm_local\Scripts\activate    # Windows PowerShell
```

---

## 3ï¸âƒ£ Gerekli KÃ¼tÃ¼phanelerin KurulmasÄ±

#### 3.1. PyTorch (GPU varsa CUDA destekli)

```bash
# Linux/macOS (CUDA 12.1 Ã¶rnek)
pip install torch==2.3.0+cu121 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121

# Windows (CUDA 12.1)
pip install torch==2.3.0+cu121 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121
```

> **CPUâ€‘only** iÃ§in: `pip install torch==2.3.0+cpu torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu`

#### 3.2. Transformers, Accelerate, HuggingFace Hub

```bash
pip install transformersaccelerate sentencepiece tqdm
```

#### 3.3. (Opsiyonel) **bitsandbytes** â€“ 4â€‘bit quantization, dÃ¼ÅŸÃ¼k VRAM tÃ¼ketimi

```bash
pip install bitsandbytes
```

> **Windows**â€™ta `bitsandbytes` kurulumu bazen hata verir. Bunun yerine `bitsandbytes-windows` (Ã¶nâ€‘derlenmiÅŸ) paketini deneyebilirsiniz:
> ```bash
> pip install bitsandbytes-windows
> ```

#### 3.4. (Opsiyonel) **PEFT** â€“ LoRAâ€‘tabanlÄ± hafif adaptasyonlar

```bash
pip install peft
```

---

## 4ï¸âƒ£ Hangi Modeli KullanacaÄŸÄ±z?  

BaÅŸlangÄ±Ã§ iÃ§in **Mistralâ€‘7Bâ€‘Instruct** (7â€¯B parametre, iyi performans).  
Quantized (GGUF) versiyonunu **bitsandbytes**/`transformers` ile doÄŸrudan CPUâ€‘only Ã§alÄ±ÅŸtÄ±rabiliriz.

| Model | HuggingFace Hub adlandÄ±rmasÄ± | Boyut (Quantized) |
|------|-----------------------------|-------------------|
| Mistralâ€‘7Bâ€‘Instruct (GGUF, 4â€‘bit) | `TheBloke/Mistral-7B-Instruct-v0.2-GGUF` | ~5â€¯GB |
| Llamaâ€‘2â€‘7Bâ€‘Chat (Q4_K_M) | `TheBloke/Llama-2-7B-Chat-GGUF` | ~5â€¯GB |
| OpenChatâ€‘3.5â€‘7B (Q4_K_M) | `TheBloke/OpenChat_3.5-1210-GGUF` | ~5â€¯GB |

> **GGUF** = â€œGPTâ€‘Quantized Unified Formatâ€. `transformers`â€¯â‰¥â€¯4.41 bu formatÄ± destekler.

### 4.1. Modeli Ä°lk Defa Ä°ndirmek

```bash
# HuggingFace Hub'dan model dosyasÄ±nÄ± (GGUF) indirelim
# â€“only the .gguf file (Ã¶rnek: mistral-7b-instruct-v0.2.Q4_K_M.gguf)

python - <<'PY'
from huggingface_hub import snapshot_download

repo_id = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
# sadece .gguf uzantÄ±lÄ± dosyayÄ± indir (en kÃ¼Ã§Ã¼k Q4_K_M sÃ¼rÃ¼mÃ¼)
local_dir = snapshot_download(repo_id,
                              allow_patterns="*.gguf",
                              resume_download=True)
print(f"Model dosyalarÄ± indirildi: {local_dir}")
PY
```

> Ä°ndirme tamamlandÄ±ÄŸÄ±nda bir klasÃ¶r iÃ§inde `*.gguf` dosyasÄ± gÃ¶receksiniz (Ã¶rnek: `mistral-7b-instruct-v0.2.Q4_K_M.gguf`).

---

## 5ï¸âƒ£ Basit Python Kodu ile **Tekâ€‘SatÄ±r** Ã‡Ä±ktÄ± Alma

AÅŸaÄŸÄ±daki kod, **transformers** pipelines APIâ€™siyle *inference* yapar.

```python
# ------------------------------------------------------------------
# 01_llm_inference.py
# ------------------------------------------------------------------
import os
from pathlib import Path

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch

# ---------- 1ï¸âƒ£ Model yolu ----------
# snapshot_download Ã§Ä±ktÄ±sÄ±ndaki klasÃ¶r yolu
model_dir = Path("~/.cache/huggingface/hub/TheBloke__Mistral-7B-Instruct-v0.2-GGUF").expanduser()
gguf_file = next(model_dir.glob("*.gguf"))   # en birinci .gguf dosyasÄ±nÄ± al
print(f"KullanÄ±lan GGUF dosyasÄ±: {gguf_file}")

# ---------- 2ï¸âƒ£ Tokenizer ----------
tokenizer = AutoTokenizer.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.2",   # aynÄ± modelin tokenizer (HFâ€™da mevcut)
    trust_remote_code=True                # bazÄ± modeller Ã¶zel kod gerektirebilir
)

# ---------- 3ï¸âƒ£ Model (bitsandbytesâ€‘4bit) ----------
# GPU varsa torch_dtype= torch.float16  + device_map="auto"
# CPU (bitsandbytes 4â€‘bit) iÃ§in `load_in_4bit=True`

model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",                       # GPU varsa otomatik daÄŸÄ±t
    trust_remote_code=True,
    # 4â€‘bit quantization (CPUâ€‘only) -----------------
    load_in_4bit=True if not torch.cuda.is_available() else False,
    # ------------------------------------------------
)

# ---------- 4ï¸âƒ£ Pipeline (text generation) ----------
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256,
    temperature=0.7,
    top_p=0.9,
    do_sample=True,
    device=0 if torch.cuda.is_available() else -1   # -1 â†’ CPU
)

# ---------- 5ï¸âƒ£ Ã‡alÄ±ÅŸtÄ±rma ----------
prompt = """Soru: TÃ¼rkiye'nin en uzun nehri hangisidir? Cevap ver ve gerekÃ§eni aÃ§Ä±kla."""
outputs = generator(prompt, num_return_sequences=1)
print("\n=== Model CevabÄ± ===")
print(outputs[0]["generated_text"])
```

### Ã‡alÄ±ÅŸtÄ±rma

```bash
python 01_llm_inference.py
```

#### Beklenen Ã‡Ä±ktÄ± (Ã¶rnek)

```
=== Model CevabÄ± ===
Soru: TÃ¼rkiye'nin en uzun nehri hangisidir? Cevap ver ve gerekÃ§eni aÃ§Ä±kla.
Cevap: TÃ¼rkiye'nin en uzun nehri KÄ±zÄ±lÄ±rmak'tÄ±r. KÄ±zÄ±lÄ±rmak, 1.355 kilometre uzunluÄŸuyla TÃ¼rkiye'nin en uzun iÃ§

...
```

> **GPU varsa** `device=0` sayesinde model GPUâ€™da (CUDA) Ã§alÄ±ÅŸÄ±r ve yanÄ±t sÃ¼resi 1â€‘2â€¯saniye civarÄ±ndadÄ±r.  
> **CPUâ€‘only** kullanÄ±mda ise 4â€‘bit quantizasyon sayesinde (â‰ˆâ€¯5â€¯GB VRAM yerine sadece RAM) 5â€‘15â€¯saniye sÃ¼rebilir â€“ hÃ¢lÃ¢ kullanÄ±ÅŸlÄ±dÄ±r.

---

## 6ï¸âƒ£ Daha GeliÅŸmiÅŸ KullanÄ±m SenaryolarÄ±  

### 6.1. Ä°nteraktif Konsol (Chatâ€‘like) DÃ¶ngÃ¼sÃ¼

```python
# ------------------------------------------------------------------
# 02_llm_chat.py
# ------------------------------------------------------------------
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_dir = "YOUR_LOCAL_MODEL_PATH"          # 4â€‘bit GGUF klasÃ¶r yolu
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")
model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
    trust_remote_code=True,
    load_in_4bit= not torch.cuda.is_available()
)

def chat_loop():
    print("ğŸš€ Yerel LLM sohbetine hoÅŸ geldiniz! (Ã§Ä±kmak iÃ§in 'quit' yazÄ±n)")
    history = []
    while True:
        user = input("\nğŸ—£ï¸ Sen: ")
        if user.strip().lower() in ("quit", "exit"):
            break
        # Sistem/Prompt formatÄ± (Mistralâ€‘Instruct tarzÄ±)
        prompt = f"""[INST] {user} [/INST]"""
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        # Generation
        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        response = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        # Model Ã§Ä±ktÄ±sÄ±nda prompt da tekrar yer alabilir â†’ sadece cevabÄ± al
        response = response.split("[/INST]")[-1].strip()
        print(f"ğŸ¤– Bot: {response}")

if __name__ == "__main__":
    chat_loop()
```

### 6.2. **LoRA** ile Modelle **Ä°nceâ€‘Ayarlama (Fineâ€‘tuning)**  

> EÄŸer kendi veri setinizle (Ã¶rnek: 1â€‘2â€¯GB metin) modeli hafifÃ§e â€œÃ¶ÄŸretmekâ€ isterseniz **PEFT** + **bitsandbytes** ile **LoRA** yaklaÅŸÄ±mÄ±nÄ± deneyebilirsiniz. AÅŸaÄŸÄ±da minimum bir ÅŸablon verilmiÅŸtir.

```bash
pip install peft datasets tqdm
```

```python
# 03_finetune_lora.py
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training

model_id = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.float16,
    trust_remote_code=True,
    load_in_4bit=True
)

# LoRA ayarlarÄ±
lora_config = LoraConfig(
    r=16,                # dÃ¼ÅŸÃ¼k rank
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # Mistralâ€™da kullanÄ±lanlar
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

# KÃ¼Ã§Ã¼k Ã¶rnek veri (JSONL, CSV vs.)
dataset = load_dataset("json", data_files={"train": "my_data/train.jsonl",
                                           "validation": "my_data/val.jsonl"})

def tokenize_fn(example):
    # Ã–rnek: {"prompt":"Soru?", "response":"Cevap"}
    text = f"[INST] {example['prompt']} [/INST] {example['response']}"
    tokenized = tokenizer(text, truncation=True, max_length=1024)
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

tokenized_ds = dataset.map(tokenize_fn, remove_columns=dataset["train"].column_names)

training_args = TrainingArguments(
    output_dir="./lora_mistral",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=3,
    fp16=True,
    logging_steps=10,
    evaluation_strategy="steps",
    eval_steps=100,
    save_steps=200,
    push_to_hub=False,
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_ds["train"],
    eval_dataset=tokenized_ds["validation"],
    tokenizer=tokenizer,
    args=training_args,
)

trainer.train()
# LoRA aÄŸÄ±rlÄ±klarÄ±nÄ± kaydedelim
model.save_pretrained("./lora_mistral")
tokenizer.save_pretrained("./lora_mistral")
```

> Fineâ€‘tuning sonrasÄ± `./lora_mistral` klasÃ¶rÃ¼ndeki **adapter** aÄŸÄ±rlÄ±klarÄ±nÄ±, aynÄ± model ile `PeftModel.from_pretrained` kullanarak tekrar yÃ¼kleyebilir ve hemen sohbet edebilirsiniz.

---

## 7ï¸âƒ£ DiÄŸer KullanÄ±ÅŸlÄ± AraÃ§lar / UI'ler  

| AraÃ§ | AÃ§Ä±klama | Kurulum (kÄ±saca) |
|------|----------|-------------------|
| **Oobabooga Text Generation Web UI** | Webâ€‘tabanlÄ±, tek komutla Ã§ok sayÄ±da model (GGUF, GPTQ, LoRA) Ã§alÄ±ÅŸtÄ±rÄ±r. | `git clone https://github.com/oobabooga/text-generation-webui && cd text-generation-webui && python -m venv venv && source venv/bin/activate && pip install -r requirements.txt && python server.py` |
| **LM Studio** (GUI, Windows/macOS/Linux) | SÃ¼rÃ¼kleâ€‘bÄ±rak ile model ekleyebilir, sohbet penceresi sunar. | <https://lmstudio.ai> sitesinden .dmg/.exe indir ve kur. |
| **vLLM** | Ã‡okâ€‘iÅŸ parÃ§acÄ±klÄ± yÃ¼ksek hÄ±zlÄ± inference (GPUâ€¯â‰¥â€¯12â€¯GB). | `pip install vllm && python -m vllm.entrypoints.openai.api_server --model TheBloke/Mistral-7B-Instruct-v0.2-GGUF` |
| **FastChat** | LLM APIâ€‘leri (OpenAIâ€‘style) sunar, birden Ã§ok model aynÄ± anda yÃ¶netilebilir. | `git clone https://github.com/lm-sys/FastChat && cd FastChat && pip install -e . && python -m fastchat.serve.controller & python -m fastchat.serve.model_worker --model-dir ~/.cache/huggingface/hub/...` |

---

## 8ï¸âƒ£ YaygÄ±n Hata ve Ã‡Ã¶zÃ¼m Ã–zetleri  

| Hata | Neden | Ã‡Ã¶zÃ¼m |
|------|-------|-------|
| `RuntimeError: CUDA out of memory` | Model VRAMâ€™i aÅŸÄ±yor. | 1) Daha kÃ¼Ã§Ã¼k model (4â€‘bit/8â€‘bit). 2) `device_map="auto"` yerine `torch_dtype=torch.float16` + `max_memory` ayarlarÄ±. 3) `mpirun`/`accelerate launch` ile model paralel daÄŸÄ±tÄ±mÄ±. |
| `ImportError: bitsandbytes` | `bitsandbytes` uygun CUDA sÃ¼rÃ¼mÃ¼yle uyumsuz. | PyTorch ve CUDA sÃ¼rÃ¼mÃ¼nÃ¼ eÅŸleÅŸtirin; `pip uninstall bitsandbytes && pip install bitsandbytes==0.44.0` gibi sÃ¼rÃ¼m belirleyin. |
| `OSError: No such file or directory: 'tokenizer.json'` | Tokenizer bulunamadÄ±. | Modelin *config* gibi bir `tokenizer` klasÃ¶rÃ¼ yoksa, aynÄ± modelin **HF tokenizer** (`mistralai/Mistral-7B-Instruct-v0.2`) ile `from_pretrained` edin. |
| `torch.cuda.is_available() == False




# BÃ–LÃœM3

Elbette! Sana temel bir **Django projesi** oluÅŸturmak iÃ§in gerekli olan baÅŸlangÄ±Ã§ adÄ±mlarÄ±nÄ±, dizin yapÄ±sÄ±nÄ±, temel **Model**, **View** ve **URL yapÄ±landÄ±rmasÄ±** ile birlikte ayrÄ±ntÄ±lÄ± olarak aÃ§Ä±klÄ±yorum.

---

## ğŸ¯ Django Projesi OluÅŸturmak iÃ§in BaÅŸlangÄ±Ã§

### 1. ğŸ› ï¸ Gerekli Ortam Kurulumu

Ä°lk olarak Python ve `pip` yÃ¼klÃ¼ olmalÄ±.

```bash
# Sanal ortam oluÅŸtur
python -m venv myenv

# Sanal ortamÄ± aktif et (Linux/Mac)
source myenv/bin/activate

# Sanal ortamÄ± aktif et (Windows)
myenv\Scripts\activate
```

### 2. ğŸ“¦ Django Kurulumu

```bash
pip install django
```

---

## ğŸ“ 1. Django Projesi ve UygulamasÄ± OluÅŸturma

```bash
# Yeni bir Django projesi oluÅŸtur
django-admin startproject myproject

# Projeye gir
cd myproject

# Bir uygulama oluÅŸtur (Ã¶rn. blog)
python manage.py startapp blog
```

---

## ğŸ“ 2. Proje Dizin YapÄ±sÄ±

AÅŸaÄŸÄ±daki gibi bir dizin yapÄ±sÄ± oluÅŸur:

```
myproject/
â”‚
â”œâ”€â”€ manage.py
â”œâ”€â”€ myproject/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py
â”‚   â”œâ”€â”€ urls.py
â”‚   â”œâ”€â”€ wsgi.py
â”‚   â””â”€â”€ asgi.py
â”‚
â””â”€â”€ blog/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ admin.py
    â”œâ”€â”€ apps.py
    â”œâ”€â”€ models.py
    â”œâ”€â”€ views.py
    â”œâ”€â”€ urls.py     # (Bu dosyayÄ± biz oluÅŸturacaÄŸÄ±z)
    â”œâ”€â”€ migrations/
    â””â”€â”€ tests.py
```

---

## ğŸ“¦ 3. UygulamayÄ± TanÄ±t (settings.py)

`myproject/settings.py` dosyasÄ±na uygulamayÄ± ekle:

```python
INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'blog',  # ğŸ‘ˆ Uygulama burada eklenir
]
```

---

## ğŸ§± 4. Model OluÅŸturma (models.py)

`blog/models.py` iÃ§eriÄŸi:

```python
from django.db import models

class Post(models.Model):
    title = models.CharField(max_length=200)
    content = models.TextField()
    created_at = models.DateTimeField(auto_now_add=True)

    def __str__(self):
        return self.title
```

### Modeli veritabanÄ±na ekle:

```bash
python manage.py makemigrations
python manage.py migrate
```

---

## ğŸ‘ï¸ 5. View OluÅŸturma (views.py)

`blog/views.py` iÃ§eriÄŸi:

```python
from django.shortcuts import render
from django.http import HttpResponse
from .models import Post

def post_list(request):
    posts = Post.objects.all()
    return render(request, 'blog/post_list.html', {'posts': posts})

def home(request):
    return HttpResponse("HoÅŸ geldiniz! Bu Django anasayfasÄ±dÄ±r.")
```

---

## ğŸ”— 6. URL YapÄ±landÄ±rmasÄ±

### a) `blog/urls.py` dosyasÄ±nÄ± oluÅŸtur:

```python
# blog/urls.py
from django.urls import path
from . import views

urlpatterns = [
    path('', views.home, name='home'),
    path('posts/', views.post_list, name='post_list'),
]
```

### b) Ana URL yapÄ±landÄ±rmasÄ± (`myproject/urls.py`):

```python
# myproject/urls.py
from django.contrib import admin
from django.urls import path, include

urlpatterns = [
    path('admin/', admin.site.urls),
    path('', include('blog.urls')),
]
```

---

## ğŸ–¼ï¸ 7. Template OluÅŸturma (HTML SayfalarÄ±)

### a) `blog/templates/blog/` klasÃ¶rÃ¼nÃ¼ oluÅŸtur:

```
blog/
â””â”€â”€ templates/
    â””â”€â”€ blog/
        â””â”€â”€ post_list.html
```

### b) `blog/templates/blog/post_list.html` iÃ§eriÄŸi:

```html
<!DOCTYPE html>
<html>
<head>
    <title>Blog YazÄ±larÄ±</title>
</head>
<body>
    <h1>Blog YazÄ±larÄ±</h1>
    <ul>
        {% for post in posts %}
            <li>{{ post.title }} - {{ post.created_at }}</li>
        {% endfor %}
    </ul>
</body>
</html>
```

### c) `settings.py` iÃ§ine template ayarlarÄ±nÄ± ekle (gerekirse):

```python
TEMPLATES = [
    {
        ...
        'DIRS': [BASE_DIR / 'templates'],  # EÄŸer genel template klasÃ¶rÃ¼ varsa
        ...
    },
]
```

---

## ğŸ§ª 8. Admin Paneline Model Ekleme

`blog/admin.py` dosyasÄ±nÄ± dÃ¼zenle:

```python
from django.contrib import admin
from .models import Post

admin.site.register(Post)
```

### Admin kullanÄ±cÄ±sÄ± oluÅŸtur:

```bash
python manage.py createsuperuser
```

---

## â–¶ï¸ 9. Sunucuyu Ã‡alÄ±ÅŸtÄ±r

```bash
python manage.py runserver
```

TarayÄ±cÄ±nda `http://127.0.0.1:8000/` adresine git.

- Admin paneli iÃ§in: `http://127.0.0.1:8000/admin/`

---

## âœ… Ã–zet

| BileÅŸen      | AÃ§Ä±klama |
|--------------|----------|
| `models.py`  | VeritabanÄ± tablolarÄ±nÄ± tanÄ±mlar |
| `views.py`   | KullanÄ±cÄ± isteklerini iÅŸler |
| `urls.py`    | URL'leri view fonksiyonlarÄ±na baÄŸlar |
| Templates    | HTML sayfalarÄ±nÄ± oluÅŸturur |
| Admin Panel  | Modelleri yÃ¶netim arayÃ¼zÃ¼nden yÃ¶netir |

---

EÄŸer Ã¶zel bir proje (Ã¶rn. blog, e-ticaret, sosyal medya) Ã¼zerine kurmak istersen, ona Ã¶zel model ve view yapÄ±larÄ±nÄ± da hazÄ±rlayabilirim.

Ä°stersen bir sonraki adÄ±m olarak veri ekleme, form iÅŸlemleri veya kullanÄ±cÄ± yetkilendirme gibi konulara da geÃ§ebiliriz. YardÄ±mcÄ± olmamÄ± ister misin? ğŸ˜Š

Click to add a cell.
